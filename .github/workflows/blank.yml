# dual_coverage_study.py
import re, json, difflib
from collections import defaultdict
from datetime import datetime, timedelta

# ----- Canonical 10-unit scaffold -----
CANON = {
 "Unit I": ["Origins of Islam","Prophethood","Makkan period","Madinan period","Khilafat-e-Rashidah","Ridda wars"],
 "Unit II": ["Umayyads","administration","expansion","culture","Arabic poetry Umayyad"],
 "Unit III": ["Abbasids","Bayt al-Hikma","translation movement","fiqh codification","Mu'tazila","Barmakids"],
 "Unit IV": ["Arabs in Spain","Al-Andalus","Umayyads of Cordoba","Taifa","Sicily"],
 "Unit V": ["Ottomans","institutions","millet","Timar","Tanzimat","Caliphate abolition"],
 "Unit VI": ["Fatimids","Ayyubids","Mamluks","Maghreb dynasties","Seljuks","Safavids"],
 "Unit VII": ["Islam in India","Delhi Sultanate","Mughals","administration","scholars in India"],
 "Unit VIII": ["Quran","Ulum al-Quran","Hadith","Ulum al-Hadith","Isnad","Matn","Compilation"],
 "Unit IX": ["Kalam","Falsafa","Al-Ghazali","Ibn Rushd","Sufism","Tariqas"],
 "Unit X": ["Modern movements","Reform","Islamic modernism","Contemporary issues","Islamic finance"]
}

# ----- Utility: chunker & safe JSON -----
def chunk_text_by_headings(text, max_chars=3000):
    chunks, buf, size = [], [], 0
    for line in text.splitlines():
        if re.match(r"^#{1,6}\s|^\d+\.\s|^[IVX]+\.\s|^[A-Z].*:$", line):
            if buf: chunks.append("\n".join(buf)); buf=[]; size=0
        if size + len(line) > max_chars and buf:
            chunks.append("\n".join(buf)); buf=[]; size=0
        buf.append(line); size += len(line)
    if buf: chunks.append("\n".join(buf))
    return chunks

def safe_json_parse(s, require_keys=None):
    s = re.sub(r"``````", "", s).strip()
    start = s.find("{"); 
    if start == -1: raise ValueError("No JSON found")
    stack=[]; fin=None
    for i,ch in enumerate(s[start:]):
        if ch in "{[": stack.append(ch)
        elif ch in "}]":
            if not stack: continue
            o=stack.pop()
            if (o=="{" and ch!="}") or (o=="[" and ch!="]"): continue
            if not stack: fin = start+i; break
    obj = json.loads(s[start:fin+1])
    if require_keys and not all(k in obj for k in require_keys):
        raise ValueError("Missing keys")
    return obj

# ----- Stubs to replace inside Gem -----
def llm_generate(prompt, context=None):
    # Replace with Gemini call; context can include retrieved chunks
    return gemini.generate(prompt=prompt, context=context)

def embed(texts):
    # Replace with Gemini embeddings and your vector DB
    return vector_db.add(texts)

def search(query, k=6, metadata=None):
    # Replace with vector_db.similarity_search
    return vector_db.search(query, k=k, metadata=metadata)

# ----- Topic extraction and mapping -----
TOPIC_PROMPT = """Extract concise topic labels from the text. 
Return JSON: {"topics":[{"title":"","keywords":["",""]}]}"""

def extract_topics(text):
    raw = llm_generate(TOPIC_PROMPT, context=text)
    data = safe_json_parse(raw, ["topics"])
    return data["topics"]

def map_topic_to_unit(title, keywords):
    title_low = title.lower()
    best_unit, best_score = None, 0.0
    for unit, canon in CANON.items():
        score = max([difflib.SequenceMatcher(None, title_low, ck.lower()).ratio() for ck in canon] + )
        kw_score = max([difflib.SequenceMatcher(None, " ".join(keywords).lower(), ck.lower()).ratio() for ck in canon] + )
        score = max(score, kw_score)
        if score > best_score:
            best_unit, best_score = unit, score
    return best_unit or "Unit X"

def detect_gaps(covered_map):
    gaps = defaultdict(list)
    for unit, canon in CANON.items():
        covered_join = " ".join(covered_map.get(unit, [])).lower()
        for ck in canon:
            if difflib.SequenceMatcher(None, covered_join, ck.lower()).ratio() < 0.35:
                gaps[unit].append(ck)
    return gaps

# ----- UGC-style generators -----
UGC_TEMPLATES = {
 "notes": "Write exam-oriented notes (400-600 words) for {topic} within {unit}.",
 "factual": "Create 8 factual MCQs (+answers). Return JSON {items:[{q,options,answer}]}",
 "concept": "Create 6 conceptual MCQs with rationale. JSON {mcqs:[{q,options,answer,why}]}",
 "comp": "Create a 140-180 word passage + 5 Qs. JSON {passage:'',questions:[{q,answer}]}",
 "chrono": "Give 6 events ordered + shuffled. JSON {ordered:[],shuffled:[]}",
 "assert": "Create 4 Assertion-Reason items. JSON {items:[{assertion,reason,answer,relation}]}",
 "match": "Create 6 matching pairs. JSON {A:[],B:[],key:{}}",
 "msq": "Create 6 multi-select Qs (2-3 correct). JSON {msq:[{q,options,answers,why}]}"
}

def gen_with_context(unit, topic, retrieved_texts):
    outputs = {}
    for key, instr in UGC_TEMPLATES.items():
        prompt = f"Context:\n{retrieved_texts}\n\nTask for UGC NET:\n{instr.format(unit=unit, topic=topic)}"
        raw = llm_generate(prompt, context=retrieved_texts)
        try:
            outputs[key] = safe_json_parse(raw)
        except:
            raw2 = llm_generate("Return VALID JSON only. " + prompt, context=retrieved_texts)
            outputs[key] = safe_json_parse(raw2)
    return outputs

def gen_without_context(unit, topic):
    outputs = {}
    for key, instr in UGC_TEMPLATES.items():
        prompt = f"Task for UGC NET, no external context available:\nUnit: {unit}\nTopic: {topic}\n{instr.format(unit=unit, topic=topic)}"
        raw = llm_generate(prompt)
        try:
            outputs[key] = safe_json_parse(raw)
        except:
            raw2 = llm_generate("Return VALID JSON only. " + prompt)
            outputs[key] = safe_json_parse(raw2)
    return outputs

# ----- SRS scheduler -----
class Card:
    def __init__(self, cid, ease=2.5, interval=0, reps=0, due=None):
        self.cid=cid; self.ease=ease; self.interval=interval; self.reps=reps
        self.due = due or datetime.utcnow()

def review(card, quality):
    if quality < 2:
        card.reps=0; card.interval=1
    else:
        card.reps+=1
        card.interval = 1 if card.reps==1 else 6 if card.reps==2 else int(round(card.interval*card.ease))
    if quality==3: card.ease += 0.15
    elif quality==1: card.ease = max(1.3, card.ease-0.15)
    card.due = datetime.utcnow() + timedelta(days=card.interval)
    return card

def flashcards_from(outputs, unit, topic):
    cards=[]
    for i, item in enumerate(outputs.get("factual",{}).get("items",[])):
        cards.append({"id": f"{unit}:{topic}:fact:{i}", "front": item.get("q",""), "back": item.get("answer","")})
    return cards

# ----- Orchestrator that merges uploaded coverage and gap coverage -----
class DualCoverageOrchestrator:
    def __init__(self):
        self.covered = defaultdict(list)              # unit -> topic titles (from uploads)
        self.packs = defaultdict(lambda: defaultdict(dict))  # unit->topic->outputs
        self.cards = {}

    def ingest_uploads(self, files_text_dict):
        # files_text_dict: {filename: text}
        all_text = []
        for fname, text in files_text_dict.items():
            for ch in chunk_text_by_headings(text):
                all_text.append(ch)
                for t in extract_topics(ch):
                    unit = map_topic_to_unit(t["title"], t.get("keywords",[]))
                    self.covered[unit].append(t["title"])
        # index all chunks for RAG
        embed(all_text)

    def prepare_unit(self, unit):
        # For topics found in uploads → RAG; for pending canon topics → gap gen
        found_topics = sorted(set(self.covered.get(unit, [])))
        canon_topics = CANON[unit]

        # RAG path for found topics
        for t in found_topics:
            retrieved = search(f"{unit} {t}", k=6)
            retrieved_text = "\n\n".join([r.text for r in retrieved])
            outputs = gen_with_context(unit, t, retrieved_text)
            self.packs[unit][t] = outputs
            for fc in flashcards_from(outputs, unit, t):
                self.cards[fc["id"]] = Card(fc["id"])

        # GAP path for pending canon topics
        pending = [c for c in canon_topics if c not in found_topics]
        for t in pending:
            outputs = gen_without_context(unit, t)
            self.packs[unit][t] = outputs
            for fc in flashcards_from(outputs, unit, t):
                self.cards[fc["id"]] = Card(fc["id"])

    def coverage_report(self):
        report={}
        for unit, canon in CANON.items():
            found = sorted(set(self.covered.get(unit, [])))
            pending = [c for c in canon if c not in found]
            report[unit] = {"covered": found, "pending": pending}
        return report

    def due_flashcards(self):
        now = datetime.utcnow()
        return [cid for cid, card in self.cards.items() if card.due <= now]

    def grade(self, card_id, quality):
        card = self.cards[card_id]
        review(card, quality)
        self.cards[card_id] = card
        return {"next_due": card.due.isoformat(), "interval_days": card.interval, "ease": round(card.ease,2)}
